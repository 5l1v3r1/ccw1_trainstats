<module 'opts' from '/u/suhubdyd/research/projects/jumble_lstm/code/auxiliary-nmt/opts.pyc'>
Namespace(batch_size=64, brnn=False, brnn_merge='concat', cnn_kernel_width=3, context_gate=None, copy_attn=False, copy_attn_force=False, coverage_attn=False, data='/data/lisatmp3/suhubdyd/multi30k.atok.low', dec_layers=1, decay_method='', decoder_type='rnn', dropout=0.3, enc_layers=2, encoder_type='rnn', epochs=17, exp='', exp_host='', feat_merge='concat', feat_vec_exponent=0.7, feat_vec_size=-1, fix_word_vecs_dec=False, fix_word_vecs_enc=False, global_attention='general', gpuid=[0], input_feed=1, kappa_dec=0.0, kappa_enc=0.0, lambda_coverage=1, layers=-1, learning_rate=1.0, learning_rate_decay=0.5, max_generator_batches=32, max_grad_norm=5, model_type='text', optim='sgd', param_init=0.1, position_encoding=False, pre_word_vecs_dec=None, pre_word_vecs_enc=None, report_every=50, rnn_size=500, rnn_type='LSTM', save_model='/data/lisatmp3/suhubdyd/models/encoder0decoder0dropout0.3wdropTrue', seed=-1, share_decoder_embeddings=False, share_embeddings=False, src_word_vec_size=500, start_checkpoint_at=0, start_decay_at=8, start_epoch=1, tgt_word_vec_size=500, train_from='', truncated_decoder=0, warmup_steps=4000, weightdropout=True, word_vec_size=-1)
('Using weight dropout', True)
Loading train and validate data from '/data/lisatmp3/suhubdyd/multi30k.atok.low'
 * number of training sentences: 29000
 * maximum batch size: 64
 * vocabulary size. source = 10841; target = 18563
Building model...
Applying weight drop of 0.3 to weight_hh_l0
Applying weight drop of 0.3 to weight_hh
Intializing model parameters.
NMTModel (
  (encoder): RNNEncoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(10841, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): WeightDrop (
      (module): LSTM(500, 500, dropout=0.3)
    )
  )
  (decoder): InputFeedRNNDecoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(18563, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): StackedLSTMWDropout (
      (dropout): Dropout (p = 0)
      (layers): ModuleList (
        (0): WeightDrop (
          (module): LSTMCell(1000, 500)
        )
      )
    )
    (attn): GlobalAttention (
      (linear_in): Linear (500 -> 500)
      (linear_out): Linear (1000 -> 500)
      (sm): Softmax ()
      (tanh): Tanh ()
    )
  )
  (generator): Sequential (
    (0): Linear (500 -> 18563)
    (1): LogSoftmax ()
  )
)
* number of parameters: 29760063
('encoder: ', 7424500)
('decoder: ', 22335563)

/u/suhubdyd/.conda/envs/lisa/lib/python2.7/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().
  result = self.forward(*input, **kwargs)
Epoch  1,    50/  454; acc:   9.45; ppl: 13223.11; 2477 src tok/s; 2561 tgt tok/s;     17 s elapsed
Epoch  1,   100/  454; acc:  14.23; ppl: 1866.02; 2975 src tok/s; 3074 tgt tok/s;     31 s elapsed
Epoch  1,   150/  454; acc:  17.02; ppl: 538.12; 3015 src tok/s; 3092 tgt tok/s;     46 s elapsed
Epoch  1,   200/  454; acc:  22.68; ppl: 247.80; 2977 src tok/s; 3118 tgt tok/s;     59 s elapsed
Epoch  1,   250/  454; acc:  24.14; ppl: 182.76; 2953 src tok/s; 3079 tgt tok/s;     73 s elapsed
Epoch  1,   300/  454; acc:  26.91; ppl: 129.88; 3042 src tok/s; 3145 tgt tok/s;     87 s elapsed
Epoch  1,   350/  454; acc:  30.32; ppl:  98.47; 2884 src tok/s; 3011 tgt tok/s;    102 s elapsed
Epoch  1,   400/  454; acc:  31.92; ppl:  76.66; 2963 src tok/s; 3073 tgt tok/s;    116 s elapsed
Epoch  1,   450/  454; acc:  33.87; ppl:  66.52; 2902 src tok/s; 3025 tgt tok/s;    130 s elapsed
Train perplexity: 320.494
Train accuracy: 23.4627
Traceback (most recent call last):
  File "train.py", line 416, in <module>
    main()
  File "train.py", line 412, in main
    train_model(model, train, valid, fields, optim)
  File "train.py", line 244, in train_model
    valid_stats = trainer.validate()
  File "/u/suhubdyd/research/projects/jumble_lstm/code/auxiliary-nmt/onmt/Trainer.py", line 172, in validate
    batch_stats = self.valid_loss.monolithic_compute_loss(batch, outputs, kappa_outputs, attns, encoder_output, kappa_encoder_output, decoder_output_wod, kappa_decoder_output_wod)
  File "/u/suhubdyd/research/projects/jumble_lstm/code/auxiliary-nmt/onmt/Loss.py", line 56, in monolithic_compute_loss
    shard_state = self.make_shard_state(batch, output, kappa_output, encoder_output, kappa_encoder_output, decoder_output_wod, kappa_decoder_output_wod, range_, attns)
  File "/u/suhubdyd/research/projects/jumble_lstm/code/auxiliary-nmt/onmt/Loss.py", line 157, in make_shard_state
    "target": batch.tgt[range_[0] + 1: range_[1]],
  File "/u/suhubdyd/.conda/envs/lisa/lib/python2.7/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/u/suhubdyd/.conda/envs/lisa/lib/python2.7/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
TypeError: slice indices must be integers or None or have an __index__ method

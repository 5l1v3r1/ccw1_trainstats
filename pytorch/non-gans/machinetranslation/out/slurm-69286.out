<module 'opts' from '/u/suhubdyd/research/projects/jumble_lstm/code/auxiliary-nmt/opts.pyc'>
Namespace(batch_size=64, brnn=False, brnn_merge='concat', cnn_kernel_width=3, context_gate=None, copy_attn=False, copy_attn_force=False, coverage_attn=False, data='/data/lisatmp3/suhubdyd/multi30k.atok.low', dec_layers=1, decay_method='', decoder_type='rnn', dropout=0.3, enc_layers=2, encoder_type='rnn', epochs=17, exp='', exp_host='', feat_merge='concat', feat_vec_exponent=0.7, feat_vec_size=-1, fix_word_vecs_dec=False, fix_word_vecs_enc=False, global_attention='general', gpuid=[0], input_feed=1, kappa_dec=0.05, kappa_enc=0.15, lambda_coverage=1, layers=-1, learning_rate=1.0, learning_rate_decay=0.5, max_generator_batches=32, max_grad_norm=5, model_type='text', optim='sgd', param_init=0.1, position_encoding=False, pre_word_vecs_dec=None, pre_word_vecs_enc=None, report_every=50, rnn_size=500, rnn_type='LSTM', save_model='/data/lisatmp3/suhubdyd/models/encoder0.15decoder0.05dropout0.3wdropTrue', seed=-1, share_decoder_embeddings=False, share_embeddings=False, src_word_vec_size=500, start_checkpoint_at=0, start_decay_at=8, start_epoch=1, tgt_word_vec_size=500, train_from='', truncated_decoder=0, warmup_steps=4000, weightdropout=True, word_vec_size=-1)
('Using Kappa L2 loss on encoder', 0.15)
('Using Kappa L2 loss on decoder', 0.05)
('Using weight dropout', True)
Loading train and validate data from '/data/lisatmp3/suhubdyd/multi30k.atok.low'
 * number of training sentences: 29000
 * maximum batch size: 64
 * vocabulary size. source = 10841; target = 18563
Building model...
Applying weight drop of 0.3 to weight_hh_l0
Applying weight drop of 0.3 to weight_hh
Intializing model parameters.
NMTModel (
  (encoder): RNNEncoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(10841, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): WeightDrop (
      (module): LSTM(500, 500, dropout=0.3)
    )
  )
  (decoder): InputFeedRNNDecoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(18563, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): StackedLSTMWDropout (
      (dropout): Dropout (p = 0)
      (layers): ModuleList (
        (0): WeightDrop (
          (module): LSTMCell(1000, 500)
        )
      )
    )
    (attn): GlobalAttention (
      (linear_in): Linear (500 -> 500)
      (linear_out): Linear (1000 -> 500)
      (sm): Softmax ()
      (tanh): Tanh ()
    )
  )
  (generator): Sequential (
    (0): Linear (500 -> 18563)
    (1): LogSoftmax ()
  )
)
* number of parameters: 29760063
('encoder: ', 7424500)
('decoder: ', 22335563)

/u/suhubdyd/.conda/envs/lisa/lib/python2.7/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().
  result = self.forward(*input, **kwargs)
Epoch  1,    50/  454; acc:   7.94; ppl: 16680.93; 4071 src tok/s; 4170 tgt tok/s;     11 s elapsed
Epoch  1,   100/  454; acc:  15.24; ppl: 2344.55; 5075 src tok/s; 5299 tgt tok/s;     19 s elapsed
Epoch  1,   150/  454; acc:  17.88; ppl: 552.75; 4952 src tok/s; 5167 tgt tok/s;     27 s elapsed
Epoch  1,   200/  454; acc:  19.97; ppl: 324.06; 5122 src tok/s; 5313 tgt tok/s;     35 s elapsed
Epoch  1,   250/  454; acc:  23.64; ppl: 182.00; 5106 src tok/s; 5260 tgt tok/s;     44 s elapsed
Epoch  1,   300/  454; acc:  28.25; ppl: 115.94; 5057 src tok/s; 5292 tgt tok/s;     52 s elapsed
Epoch  1,   350/  454; acc:  30.43; ppl:  90.82; 5091 src tok/s; 5363 tgt tok/s;     59 s elapsed
Epoch  1,   400/  454; acc:  29.02; ppl:  94.36; 5170 src tok/s; 5307 tgt tok/s;     68 s elapsed
Epoch  1,   450/  454; acc:  32.94; ppl:  68.93; 5013 src tok/s; 5200 tgt tok/s;     77 s elapsed
Train perplexity: 352.042
Train accuracy: 22.8725
Validation perplexity: 69.1038
Validation accuracy: 32.0491

Epoch  2,    50/  454; acc:  34.75; ppl:  58.42; 5004 src tok/s; 5172 tgt tok/s;      9 s elapsed
Epoch  2,   100/  454; acc:  37.63; ppl:  47.31; 5072 src tok/s; 5296 tgt tok/s;     17 s elapsed
Epoch  2,   150/  454; acc:  39.87; ppl:  39.77; 5063 src tok/s; 5235 tgt tok/s;     25 s elapsed
Epoch  2,   200/  454; acc:  43.13; ppl:  32.87; 5011 src tok/s; 5201 tgt tok/s;     33 s elapsed
Epoch  2,   250/  454; acc:  46.29; ppl:  26.24; 5077 src tok/s; 5315 tgt tok/s;     41 s elapsed
Epoch  2,   300/  454; acc:  45.66; ppl:  26.92; 5185 src tok/s; 5335 tgt tok/s;     50 s elapsed
Epoch  2,   350/  454; acc:  47.64; ppl:  23.71; 5157 src tok/s; 5314 tgt tok/s;     58 s elapsed
Epoch  2,   400/  454; acc:  50.64; ppl:  19.66; 5053 src tok/s; 5282 tgt tok/s;     66 s elapsed
Epoch  2,   450/  454; acc:  52.14; ppl:  17.52; 4934 src tok/s; 5154 tgt tok/s;     74 s elapsed
Train perplexity: 30.2688
Train accuracy: 44.1494
Validation perplexity: 17.7525
Validation accuracy: 52.313

Epoch  3,    50/  454; acc:  55.85; ppl:  13.52; 4922 src tok/s; 5168 tgt tok/s;      8 s elapsed
Epoch  3,   100/  454; acc:  52.61; ppl:  15.72; 5170 src tok/s; 5271 tgt tok/s;     17 s elapsed
Epoch  3,   150/  454; acc:  55.42; ppl:  13.75; 5092 src tok/s; 5292 tgt tok/s;     25 s elapsed
Epoch  3,   200/  454; acc:  55.87; ppl:  12.96; 5081 src tok/s; 5256 tgt tok/s;     33 s elapsed
Epoch  3,   250/  454; acc:  57.56; ppl:  11.80; 5077 src tok/s; 5325 tgt tok/s;     41 s elapsed
Epoch  3,   300/  454; acc:  56.55; ppl:  12.72; 5107 src tok/s; 5275 tgt tok/s;     50 s elapsed
Epoch  3,   350/  454; acc:  57.34; ppl:  11.99; 5089 src tok/s; 5287 tgt tok/s;     58 s elapsed
Epoch  3,   400/  454; acc:  59.18; ppl:  10.70; 4988 src tok/s; 5199 tgt tok/s;     66 s elapsed
Epoch  3,   450/  454; acc:  58.70; ppl:  10.96; 5069 src tok/s; 5264 tgt tok/s;     74 s elapsed
Train perplexity: 12.6053
Train accuracy: 56.5505
Validation perplexity: 10.6603
Validation accuracy: 59.8694

Epoch  4,    50/  454; acc:  62.00; ppl:   8.27; 5025 src tok/s; 5205 tgt tok/s;      8 s elapsed
Epoch  4,   100/  454; acc:  61.48; ppl:   8.75; 5062 src tok/s; 5238 tgt tok/s;     17 s elapsed
Epoch  4,   150/  454; acc:  61.13; ppl:   8.71; 5042 src tok/s; 5207 tgt tok/s;     25 s elapsed
Epoch  4,   200/  454; acc:  61.85; ppl:   8.38; 5141 src tok/s; 5375 tgt tok/s;     33 s elapsed
Epoch  4,   250/  454; acc:  63.07; ppl:   7.86; 5127 src tok/s; 5341 tgt tok/s;     41 s elapsed
Epoch  4,   300/  454; acc:  61.50; ppl:   8.51; 5069 src tok/s; 5243 tgt tok/s;     50 s elapsed
Epoch  4,   350/  454; acc:  62.25; ppl:   7.89; 5087 src tok/s; 5292 tgt tok/s;     58 s elapsed
Epoch  4,   400/  454; acc:  62.54; ppl:   8.05; 5096 src tok/s; 5298 tgt tok/s;     66 s elapsed
Epoch  4,   450/  454; acc:  62.41; ppl:   8.14; 4975 src tok/s; 5156 tgt tok/s;     74 s elapsed
Train perplexity: 8.27037
Train accuracy: 62.037
Validation perplexity: 8.59507
Validation accuracy: 62.7501

Epoch  5,    50/  454; acc:  65.18; ppl:   6.37; 5074 src tok/s; 5262 tgt tok/s;      8 s elapsed
Epoch  5,   100/  454; acc:  65.88; ppl:   6.07; 5133 src tok/s; 5318 tgt tok/s;     16 s elapsed
Epoch  5,   150/  454; acc:  65.60; ppl:   6.22; 5026 src tok/s; 5216 tgt tok/s;     25 s elapsed
Epoch  5,   200/  454; acc:  64.68; ppl:   6.60; 5077 src tok/s; 5262 tgt tok/s;     33 s elapsed
Epoch  5,   250/  454; acc:  66.36; ppl:   5.83; 5140 src tok/s; 5342 tgt tok/s;     41 s elapsed
Epoch  5,   300/  454; acc:  65.43; ppl:   6.29; 5068 src tok/s; 5256 tgt tok/s;     49 s elapsed
Epoch  5,   350/  454; acc:  66.00; ppl:   5.99; 5107 src tok/s; 5313 tgt tok/s;     58 s elapsed
Epoch  5,   400/  454; acc:  64.17; ppl:   6.73; 5076 src tok/s; 5262 tgt tok/s;     66 s elapsed
Epoch  5,   450/  454; acc:  66.05; ppl:   6.11; 5128 src tok/s; 5331 tgt tok/s;     74 s elapsed
Train perplexity: 6.24391
Train accuracy: 65.4741
Validation perplexity: 7.50128
Validation accuracy: 63.7222

Epoch  6,    50/  454; acc:  68.68; ppl:   4.75; 5081 src tok/s; 5288 tgt tok/s;      8 s elapsed
Epoch  6,   100/  454; acc:  68.23; ppl:   5.05; 5175 src tok/s; 5356 tgt tok/s;     16 s elapsed
Epoch  6,   150/  454; acc:  69.28; ppl:   4.73; 5104 src tok/s; 5334 tgt tok/s;     24 s elapsed
Epoch  6,   200/  454; acc:  67.30; ppl:   5.24; 5103 src tok/s; 5292 tgt tok/s;     33 s elapsed
Epoch  6,   250/  454; acc:  67.69; ppl:   5.14; 5184 src tok/s; 5392 tgt tok/s;     41 s elapsed
Epoch  6,   300/  454; acc:  67.72; ppl:   5.20; 5071 src tok/s; 5275 tgt tok/s;     49 s elapsed
Epoch  6,   350/  454; acc:  68.11; ppl:   5.02; 5100 src tok/s; 5274 tgt tok/s;     57 s elapsed
Epoch  6,   400/  454; acc:  67.96; ppl:   5.08; 4952 src tok/s; 5119 tgt tok/s;     66 s elapsed
Epoch  6,   450/  454; acc:  67.85; ppl:   5.11; 4964 src tok/s; 5148 tgt tok/s;     74 s elapsed
Train perplexity: 5.04147
Train accuracy: 68.0707
Validation perplexity: 7.0795
Validation accuracy: 65.5811

Epoch  7,    50/  454; acc:  72.49; ppl:   3.79; 5005 src tok/s; 5215 tgt tok/s;      8 s elapsed
Epoch  7,   100/  454; acc:  69.79; ppl:   4.33; 5077 src tok/s; 5247 tgt tok/s;     17 s elapsed
Epoch  7,   150/  454; acc:  70.48; ppl:   4.18; 5092 src tok/s; 5297 tgt tok/s;     25 s elapsed
Epoch  7,   200/  454; acc:  70.65; ppl:   4.12; 5040 src tok/s; 5214 tgt tok/s;     33 s elapsed
Epoch  7,   250/  454; acc:  70.29; ppl:   4.15; 5110 src tok/s; 5297 tgt tok/s;     41 s elapsed
Epoch  7,   300/  454; acc:  70.11; ppl:   4.24; 5073 src tok/s; 5252 tgt tok/s;     50 s elapsed
Epoch  7,   350/  454; acc:  69.45; ppl:   4.40; 5049 src tok/s; 5252 tgt tok/s;     58 s elapsed
Epoch  7,   400/  454; acc:  69.31; ppl:   4.51; 5068 src tok/s; 5264 tgt tok/s;     66 s elapsed
Epoch  7,   450/  454; acc:  69.29; ppl:   4.48; 5032 src tok/s; 5222 tgt tok/s;     75 s elapsed
Train perplexity: 4.23891
Train accuracy: 70.2039
Validation perplexity: 6.75683
Validation accuracy: 66.1345

Epoch  8,    50/  454; acc:  74.40; ppl:   3.21; 5104 src tok/s; 5296 tgt tok/s;      8 s elapsed
Epoch  8,   100/  454; acc:  72.10; ppl:   3.62; 5115 src tok/s; 5305 tgt tok/s;     16 s elapsed
Epoch  8,   150/  454; acc:  72.38; ppl:   3.58; 5067 src tok/s; 5270 tgt tok/s;     25 s elapsed
Epoch  8,   200/  454; acc:  72.35; ppl:   3.60; 4997 src tok/s; 5168 tgt tok/s;     33 s elapsed
Epoch  8,   250/  454; acc:  72.55; ppl:   3.54; 5050 src tok/s; 5269 tgt tok/s;     41 s elapsed
Epoch  8,   300/  454; acc:  71.01; ppl:   3.95; 5063 src tok/s; 5220 tgt tok/s;     50 s elapsed
Epoch  8,   350/  454; acc:  72.68; ppl:   3.59; 4844 src tok/s; 5057 tgt tok/s;     58 s elapsed
Epoch  8,   400/  454; acc:  70.39; ppl:   4.03; 4907 src tok/s; 5110 tgt tok/s;     67 s elapsed
Epoch  8,   450/  454; acc:  71.16; ppl:   3.87; 5044 src tok/s; 5211 tgt tok/s;     75 s elapsed
Train perplexity: 3.66189
Train accuracy: 72.0945
Validation perplexity: 6.66487
Validation accuracy: 66.6667
Decaying learning rate to 0.5

Epoch  9,    50/  454; acc:  77.54; ppl:   2.63; 4936 src tok/s; 5158 tgt tok/s;      8 s elapsed
Epoch  9,   100/  454; acc:  76.54; ppl:   2.81; 5161 src tok/s; 5301 tgt tok/s;     17 s elapsed
Epoch  9,   150/  454; acc:  77.38; ppl:   2.73; 5155 src tok/s; 5331 tgt tok/s;     25 s elapsed
Epoch  9,   200/  454; acc:  77.34; ppl:   2.70; 4987 src tok/s; 5192 tgt tok/s;     33 s elapsed
Epoch  9,   250/  454; acc:  77.11; ppl:   2.70; 4975 src tok/s; 5169 tgt tok/s;     42 s elapsed
Epoch  9,   300/  454; acc:  76.97; ppl:   2.72; 5149 src tok/s; 5332 tgt tok/s;     50 s elapsed
Epoch  9,   350/  454; acc:  77.16; ppl:   2.69; 5173 src tok/s; 5393 tgt tok/s;     58 s elapsed
Epoch  9,   400/  454; acc:  76.58; ppl:   2.83; 5124 src tok/s; 5339 tgt tok/s;     66 s elapsed
Epoch  9,   450/  454; acc:  76.79; ppl:   2.75; 5075 src tok/s; 5253 tgt tok/s;     74 s elapsed
Train perplexity: 2.72757
Train accuracy: 77.0576
Validation perplexity: 6.04896
Validation accuracy: 68.9229
Decaying learning rate to 0.25

Epoch 10,    50/  454; acc:  80.88; ppl:   2.26; 5095 src tok/s; 5272 tgt tok/s;      8 s elapsed
Epoch 10,   100/  454; acc:  81.38; ppl:   2.17; 5091 src tok/s; 5279 tgt tok/s;     16 s elapsed
Epoch 10,   150/  454; acc:  81.20; ppl:   2.18; 4977 src tok/s; 5175 tgt tok/s;     25 s elapsed
Epoch 10,   200/  454; acc:  80.32; ppl:   2.32; 5002 src tok/s; 5184 tgt tok/s;     33 s elapsed
Epoch 10,   250/  454; acc:  79.70; ppl:   2.38; 5155 src tok/s; 5338 tgt tok/s;     42 s elapsed
Epoch 10,   300/  454; acc:  81.81; ppl:   2.14; 5187 src tok/s; 5392 tgt tok/s;     49 s elapsed
Epoch 10,   350/  454; acc:  80.75; ppl:   2.25; 5025 src tok/s; 5218 tgt tok/s;     58 s elapsed
Epoch 10,   400/  454; acc:  80.80; ppl:   2.23; 5070 src tok/s; 5288 tgt tok/s;     66 s elapsed
Epoch 10,   450/  454; acc:  80.44; ppl:   2.28; 5096 src tok/s; 5287 tgt tok/s;     74 s elapsed
Train perplexity: 2.2449
Train accuracy: 80.8088
Validation perplexity: 6.19239
Validation accuracy: 69.2422
Decaying learning rate to 0.125

Epoch 11,    50/  454; acc:  83.42; ppl:   1.98; 5012 src tok/s; 5200 tgt tok/s;      8 s elapsed
Epoch 11,   100/  454; acc:  82.88; ppl:   2.04; 5088 src tok/s; 5264 tgt tok/s;     17 s elapsed
Epoch 11,   150/  454; acc:  82.71; ppl:   2.05; 5210 src tok/s; 5376 tgt tok/s;     25 s elapsed
Epoch 11,   200/  454; acc:  83.51; ppl:   1.98; 4989 src tok/s; 5193 tgt tok/s;     33 s elapsed
Epoch 11,   250/  454; acc:  82.43; ppl:   2.09; 5124 src tok/s; 5299 tgt tok/s;     41 s elapsed
Epoch 11,   300/  454; acc:  83.42; ppl:   1.99; 5078 src tok/s; 5310 tgt tok/s;     50 s elapsed
Epoch 11,   350/  454; acc:  83.28; ppl:   2.02; 5045 src tok/s; 5218 tgt tok/s;     58 s elapsed
Epoch 11,   400/  454; acc:  82.94; ppl:   2.04; 5033 src tok/s; 5250 tgt tok/s;     66 s elapsed
Epoch 11,   450/  454; acc:  82.88; ppl:   2.06; 5139 src tok/s; 5353 tgt tok/s;     74 s elapsed
Train perplexity: 2.03152
Train accuracy: 83.0168
Validation perplexity: 6.3475
Validation accuracy: 69.2209
Decaying learning rate to 0.0625

Epoch 12,    50/  454; acc:  85.34; ppl:   1.82; 5127 src tok/s; 5347 tgt tok/s;      8 s elapsed
Epoch 12,   100/  454; acc:  83.21; ppl:   2.04; 5156 src tok/s; 5321 tgt tok/s;     16 s elapsed
Epoch 12,   150/  454; acc:  83.60; ppl:   1.99; 5027 src tok/s; 5202 tgt tok/s;     25 s elapsed
Epoch 12,   200/  454; acc:  84.27; ppl:   1.91; 5151 src tok/s; 5359 tgt tok/s;     33 s elapsed
Epoch 12,   250/  454; acc:  83.78; ppl:   1.97; 5071 src tok/s; 5237 tgt tok/s;     42 s elapsed
Epoch 12,   300/  454; acc:  84.42; ppl:   1.89; 5098 src tok/s; 5323 tgt tok/s;     49 s elapsed
Epoch 12,   350/  454; acc:  84.01; ppl:   1.94; 5291 src tok/s; 5446 tgt tok/s;     58 s elapsed
Epoch 12,   400/  454; acc:  84.69; ppl:   1.87; 5187 src tok/s; 5429 tgt tok/s;     65 s elapsed
Epoch 12,   450/  454; acc:  83.77; ppl:   1.94; 4965 src tok/s; 5158 tgt tok/s;     74 s elapsed
Train perplexity: 1.93019
Train accuracy: 84.1008
Validation perplexity: 6.44889
Validation accuracy: 69.4196
Decaying learning rate to 0.03125

slurmstepd-leto50: error: *** JOB 69286 ON leto50 CANCELLED AT 2017-10-26T19:18:37 DUE TO PREEMPTION ***
<module 'opts' from '/u/suhubdyd/research/projects/jumble_lstm/code/auxiliary-nmt/opts.pyc'>
Namespace(batch_size=64, brnn=False, brnn_merge='concat', cnn_kernel_width=3, context_gate=None, copy_attn=False, copy_attn_force=False, coverage_attn=False, data='/data/lisatmp3/suhubdyd/multi30k.atok.low', dec_layers=1, decay_method='', decoder_type='rnn', dropout=0.3, enc_layers=2, encoder_type='rnn', epochs=17, exp='', exp_host='', feat_merge='concat', feat_vec_exponent=0.7, feat_vec_size=-1, fix_word_vecs_dec=False, fix_word_vecs_enc=False, global_attention='general', gpuid=[0], input_feed=1, kappa_dec=0.05, kappa_enc=0.15, lambda_coverage=1, layers=-1, learning_rate=1.0, learning_rate_decay=0.5, max_generator_batches=32, max_grad_norm=5, model_type='text', optim='sgd', param_init=0.1, position_encoding=False, pre_word_vecs_dec=None, pre_word_vecs_enc=None, report_every=50, rnn_size=500, rnn_type='LSTM', save_model='/data/lisatmp3/suhubdyd/models/encoder0.15decoder0.05dropout0.3wdropTrue', seed=-1, share_decoder_embeddings=False, share_embeddings=False, src_word_vec_size=500, start_checkpoint_at=0, start_decay_at=8, start_epoch=1, tgt_word_vec_size=500, train_from='', truncated_decoder=0, warmup_steps=4000, weightdropout=True, word_vec_size=-1)
('Using Kappa L2 loss on encoder', 0.15)
('Using Kappa L2 loss on decoder', 0.05)
('Using weight dropout', True)
Loading train and validate data from '/data/lisatmp3/suhubdyd/multi30k.atok.low'
 * number of training sentences: 29000
 * maximum batch size: 64
 * vocabulary size. source = 10841; target = 18563
Building model...
Applying weight drop of 0.3 to weight_hh_l0
Applying weight drop of 0.3 to weight_hh
Intializing model parameters.
NMTModel (
  (encoder): RNNEncoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(10841, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): WeightDrop (
      (module): LSTM(500, 500, dropout=0.3)
    )
  )
  (decoder): InputFeedRNNDecoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(18563, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): StackedLSTMWDropout (
      (dropout): Dropout (p = 0)
      (layers): ModuleList (
        (0): WeightDrop (
          (module): LSTMCell(1000, 500)
        )
      )
    )
    (attn): GlobalAttention (
      (linear_in): Linear (500 -> 500)
      (linear_out): Linear (1000 -> 500)
      (sm): Softmax ()
      (tanh): Tanh ()
    )
  )
  (generator): Sequential (
    (0): Linear (500 -> 18563)
    (1): LogSoftmax ()
  )
)
* number of parameters: 29760063
('encoder: ', 7424500)
('decoder: ', 22335563)

/u/suhubdyd/.conda/envs/lisa/lib/python2.7/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().
  result = self.forward(*input, **kwargs)
Epoch  1,    50/  454; acc:   7.99; ppl: 25481.21; 4869 src tok/s; 5057 tgt tok/s;      9 s elapsed
slurmstepd-leto30: error: *** JOB 69286 ON leto30 CANCELLED AT 2017-10-26T19:22:46 DUE TO PREEMPTION ***
<module 'opts' from '/u/suhubdyd/research/projects/jumble_lstm/code/auxiliary-nmt/opts.pyc'>
Namespace(batch_size=64, brnn=False, brnn_merge='concat', cnn_kernel_width=3, context_gate=None, copy_attn=False, copy_attn_force=False, coverage_attn=False, data='/data/lisatmp3/suhubdyd/multi30k.atok.low', dec_layers=1, decay_method='', decoder_type='rnn', dropout=0.3, enc_layers=2, encoder_type='rnn', epochs=17, exp='', exp_host='', feat_merge='concat', feat_vec_exponent=0.7, feat_vec_size=-1, fix_word_vecs_dec=False, fix_word_vecs_enc=False, global_attention='general', gpuid=[0], input_feed=1, kappa_dec=0.05, kappa_enc=0.15, lambda_coverage=1, layers=-1, learning_rate=1.0, learning_rate_decay=0.5, max_generator_batches=32, max_grad_norm=5, model_type='text', optim='sgd', param_init=0.1, position_encoding=False, pre_word_vecs_dec=None, pre_word_vecs_enc=None, report_every=50, rnn_size=500, rnn_type='LSTM', save_model='/data/lisatmp3/suhubdyd/models/encoder0.15decoder0.05dropout0.3wdropTrue', seed=-1, share_decoder_embeddings=False, share_embeddings=False, src_word_vec_size=500, start_checkpoint_at=0, start_decay_at=8, start_epoch=1, tgt_word_vec_size=500, train_from='', truncated_decoder=0, warmup_steps=4000, weightdropout=True, word_vec_size=-1)
('Using Kappa L2 loss on encoder', 0.15)
('Using Kappa L2 loss on decoder', 0.05)
('Using weight dropout', True)
Loading train and validate data from '/data/lisatmp3/suhubdyd/multi30k.atok.low'
 * number of training sentences: 29000
 * maximum batch size: 64
 * vocabulary size. source = 10841; target = 18563
Building model...
Applying weight drop of 0.3 to weight_hh_l0
Applying weight drop of 0.3 to weight_hh
Intializing model parameters.
NMTModel (
  (encoder): RNNEncoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(10841, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): WeightDrop (
      (module): LSTM(500, 500, dropout=0.3)
    )
  )
  (decoder): InputFeedRNNDecoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(18563, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): StackedLSTMWDropout (
      (dropout): Dropout (p = 0)
      (layers): ModuleList (
        (0): WeightDrop (
          (module): LSTMCell(1000, 500)
        )
      )
    )
    (attn): GlobalAttention (
      (linear_in): Linear (500 -> 500)
      (linear_out): Linear (1000 -> 500)
      (sm): Softmax ()
      (tanh): Tanh ()
    )
  )
  (generator): Sequential (
    (0): Linear (500 -> 18563)
    (1): LogSoftmax ()
  )
)
* number of parameters: 29760063
('encoder: ', 7424500)
('decoder: ', 22335563)

/u/suhubdyd/.conda/envs/lisa/lib/python2.7/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().
  result = self.forward(*input, **kwargs)
Epoch  1,    50/  454; acc:   6.94; ppl: 23139.25; 5818 src tok/s; 6002 tgt tok/s;      7 s elapsed
Epoch  1,   100/  454; acc:  14.15; ppl: 1251.04; 6407 src tok/s; 6686 tgt tok/s;     14 s elapsed
Epoch  1,   150/  454; acc:  18.35; ppl: 479.98; 6392 src tok/s; 6657 tgt tok/s;     20 s elapsed
Epoch  1,   200/  454; acc:  20.77; ppl: 296.80; 6359 src tok/s; 6618 tgt tok/s;     27 s elapsed
Epoch  1,   250/  454; acc:  24.97; ppl: 176.41; 6389 src tok/s; 6643 tgt tok/s;     33 s elapsed
Epoch  1,   300/  454; acc:  26.33; ppl: 136.65; 6430 src tok/s; 6649 tgt tok/s;     40 s elapsed
Epoch  1,   350/  454; acc:  29.33; ppl:  96.57; 6490 src tok/s; 6746 tgt tok/s;     47 s elapsed
Epoch  1,   400/  454; acc:  31.46; ppl:  80.52; 6210 src tok/s; 6461 tgt tok/s;     53 s elapsed
Epoch  1,   450/  454; acc:  33.35; ppl:  66.40; 6450 src tok/s; 6654 tgt tok/s;     60 s elapsed
Train perplexity: 334.213
Train accuracy: 22.8672
Validation perplexity: 53.5188
Validation accuracy: 36.2778

Epoch  2,    50/  454; acc:  36.11; ppl:  51.93; 6393 src tok/s; 6656 tgt tok/s;      6 s elapsed
Epoch  2,   100/  454; acc:  36.98; ppl:  49.80; 6476 src tok/s; 6710 tgt tok/s;     13 s elapsed
Epoch  2,   150/  454; acc:  39.84; ppl:  41.78; 6340 src tok/s; 6518 tgt tok/s;     20 s elapsed
Epoch  2,   200/  454; acc:  43.49; ppl:  32.16; 6422 src tok/s; 6697 tgt tok/s;     26 s elapsed
Epoch  2,   250/  454; acc:  45.64; ppl:  28.30; 6273 src tok/s; 6568 tgt tok/s;     33 s elapsed
Epoch  2,   300/  454; acc:  46.94; ppl:  25.37; 6327 src tok/s; 6555 tgt tok/s;     40 s elapsed
Epoch  2,   350/  454; acc:  48.36; ppl:  22.86; 6390 src tok/s; 6630 tgt tok/s;     46 s elapsed
Epoch  2,   400/  454; acc:  51.20; ppl:  19.51; 6299 src tok/s; 6544 tgt tok/s;     53 s elapsed
Epoch  2,   450/  454; acc:  51.28; ppl:  19.41; 6285 src tok/s; 6505 tgt tok/s;     59 s elapsed
Train perplexity: 30.2316
Train accuracy: 44.4533
Validation perplexity: 16.6399
Validation accuracy: 53.2141

Epoch  3,    50/  454; acc:  55.53; ppl:  13.74; 6207 src tok/s; 6478 tgt tok/s;      6 s elapsed
Epoch  3,   100/  454; acc:  53.80; ppl:  15.50; 6560 src tok/s; 6747 tgt tok/s;     13 s elapsed
Epoch  3,   150/  454; acc:  54.61; ppl:  14.52; 6372 src tok/s; 6599 tgt tok/s;     20 s elapsed
Epoch  3,   200/  454; acc:  56.70; ppl:  12.85; 6330 src tok/s; 6602 tgt tok/s;     26 s elapsed
Epoch  3,   250/  454; acc:  56.89; ppl:  12.45; 6245 src tok/s; 6497 tgt tok/s;     33 s elapsed
Epoch  3,   300/  454; acc:  57.68; ppl:  11.89; 6474 src tok/s; 6696 tgt tok/s;     40 s elapsed
Epoch  3,   350/  454; acc:  57.82; ppl:  11.47; 6377 src tok/s; 6593 tgt tok/s;     46 s elapsed
Epoch  3,   400/  454; acc:  58.90; ppl:  10.92; 6153 src tok/s; 6443 tgt tok/s;     53 s elapsed
Epoch  3,   450/  454; acc:  59.26; ppl:  10.60; 6333 src tok/s; 6553 tgt tok/s;     59 s elapsed
Train perplexity: 12.5816
Train accuracy: 56.767
Validation perplexity: 9.9662
Validation accuracy: 60.6641

Epoch  4,    50/  454; acc:  60.76; ppl:   8.85; 6144 src tok/s; 6395 tgt tok/s;      7 s elapsed
Epoch  4,   100/  454; acc:  62.46; ppl:   8.15; 6452 src tok/s; 6691 tgt tok/s;     13 s elapsed
Epoch  4,   150/  454; acc:  61.46; ppl:   8.67; 6362 src tok/s; 6571 tgt tok/s;     20 s elapsed
Epoch  4,   200/  454; acc:  62.60; ppl:   8.06; 6396 src tok/s; 6663 tgt tok/s;     26 s elapsed
Epoch  4,   250/  454; acc:  60.31; ppl:   9.19; 6556 src tok/s; 6709 tgt tok/s;     33 s elapsed
Epoch  4,   300/  454; acc:  63.99; ppl:   7.22; 6242 src tok/s; 6575 tgt tok/s;     40 s elapsed
Epoch  4,   350/  454; acc:  62.54; ppl:   8.14; 6290 src tok/s; 6542 tgt tok/s;     46 s elapsed
Epoch  4,   400/  454; acc:  63.19; ppl:   7.81; 5689 src tok/s; 5891 tgt tok/s;     54 s elapsed
Epoch  4,   450/  454; acc:  63.01; ppl:   7.99; 6119 src tok/s; 6366 tgt tok/s;     60 s elapsed
Train perplexity: 8.2406
Train accuracy: 62.1982
Validation perplexity: 8.35249
Validation accuracy: 62.5798

Epoch  5,    50/  454; acc:  65.54; ppl:   6.28; 6167 src tok/s; 6383 tgt tok/s;      7 s elapsed
Epoch  5,   100/  454; acc:  65.80; ppl:   6.13; 6438 src tok/s; 6705 tgt tok/s;     13 s elapsed
Epoch  5,   150/  454; acc:  65.33; ppl:   6.41; 6356 src tok/s; 6613 tgt tok/s;     20 s elapsed
Epoch  5,   200/  454; acc:  66.20; ppl:   5.98; 6371 src tok/s; 6614 tgt tok/s;     27 s elapsed
Epoch  5,   250/  454; acc:  66.00; ppl:   6.23; 6224 src tok/s; 6440 tgt tok/s;     33 s elapsed
Epoch  5,   300/  454; acc:  65.37; ppl:   6.33; 6425 src tok/s; 6665 tgt tok/s;     40 s elapsed
Epoch  5,   350/  454; acc:  66.99; ppl:   5.73; 6298 src tok/s; 6610 tgt tok/s;     46 s elapsed
Epoch  5,   400/  454; acc:  64.84; ppl:   6.50; 6536 src tok/s; 6674 tgt tok/s;     53 s elapsed
Epoch  5,   450/  454; acc:  65.59; ppl:   6.24; 6175 src tok/s; 6451 tgt tok/s;     60 s elapsed
Train perplexity: 6.1969
Train accuracy: 65.7367
Validation perplexity: 7.2902
Validation accuracy: 65.3257

Epoch  6,    50/  454; acc:  69.19; ppl:   4.71; 6141 src tok/s; 6449 tgt tok/s;      7 s elapsed
Epoch  6,   100/  454; acc:  68.03; ppl:   5.06; 6382 src tok/s; 6607 tgt tok/s;     13 s elapsed
Epoch  6,   150/  454; acc:  68.12; ppl:   4.96; 6422 src tok/s; 6640 tgt tok/s;     20 s elapsed
Epoch  6,   200/  454; acc:  68.46; ppl:   4.99; 6254 src tok/s; 6520 tgt tok/s;     27 s elapsed
Epoch  6,   250/  454; acc:  68.24; ppl:   5.02; 6469 src tok/s; 6663 tgt tok/s;     33 s elapsed
Epoch  6,   300/  454; acc:  68.56; ppl:   4.89; 6327 src tok/s; 6587 tgt tok/s;     40 s elapsed
Epoch  6,   350/  454; acc:  67.84; ppl:   5.12; 6275 src tok/s; 6516 tgt tok/s;     46 s elapsed
Epoch  6,   400/  454; acc:  68.18; ppl:   5.04; 6289 src tok/s; 6520 tgt tok/s;     53 s elapsed
Epoch  6,   450/  454; acc:  68.02; ppl:   4.99; 6253 src tok/s; 6479 tgt tok/s;     60 s elapsed
Train perplexity: 4.98913
Train accuracy: 68.2371
Validation perplexity: 7.29769
Validation accuracy: 64.2259
Decaying learning rate to 0.5

Epoch  7,    50/  454; acc:  73.81; ppl:   3.50; 6277 src tok/s; 6513 tgt tok/s;      7 s elapsed
Epoch  7,   100/  454; acc:  73.27; ppl:   3.59; 6344 src tok/s; 6583 tgt tok/s;     13 s elapsed
Epoch  7,   150/  454; acc:  73.80; ppl:   3.56; 6368 src tok/s; 6609 tgt tok/s;     20 s elapsed
Epoch  7,   200/  454; acc:  73.58; ppl:   3.51; 6335 src tok/s; 6603 tgt tok/s;     26 s elapsed
Epoch  7,   250/  454; acc:  72.96; ppl:   3.68; 6486 src tok/s; 6671 tgt tok/s;     33 s elapsed
Epoch  7,   300/  454; acc:  73.80; ppl:   3.48; 6195 src tok/s; 6483 tgt tok/s;     40 s elapsed
Epoch  7,   350/  454; acc:  72.69; ppl:   3.71; 6304 src tok/s; 6531 tgt tok/s;     46 s elapsed
Epoch  7,   400/  454; acc:  74.09; ppl:   3.45; 6265 src tok/s; 6495 tgt tok/s;     53 s elapsed
Epoch  7,   450/  454; acc:  73.02; ppl:   3.65; 6170 src tok/s; 6405 tgt tok/s;     60 s elapsed
Train perplexity: 3.57087
Train accuracy: 73.4364
Validation perplexity: 6.17933
Validation accuracy: 68.3199
Decaying learning rate to 0.25

Epoch  8,    50/  454; acc:  76.67; ppl:   2.95; 6123 src tok/s; 6366 tgt tok/s;      7 s elapsed
Epoch  8,   100/  454; acc:  77.80; ppl:   2.77; 6250 src tok/s; 6520 tgt tok/s;     13 s elapsed
Epoch  8,   150/  454; acc:  78.04; ppl:   2.73; 6362 src tok/s; 6626 tgt tok/s;     20 s elapsed
Epoch  8,   200/  454; acc:  76.30; ppl:   2.96; 6373 src tok/s; 6573 tgt tok/s;     27 s elapsed
Epoch  8,   250/  454; acc:  77.75; ppl:   2.78; 6254 src tok/s; 6542 tgt tok/s;     33 s elapsed
Epoch  8,   300/  454; acc:  76.84; ppl:   2.94; 6554 src tok/s; 6737 tgt tok/s;     40 s elapsed
Epoch  8,   350/  454; acc:  76.47; ppl:   2.98; 6349 src tok/s; 6557 tgt tok/s;     47 s elapsed
Epoch  8,   400/  454; acc:  77.32; ppl:   2.80; 6266 src tok/s; 6529 tgt tok/s;     53 s elapsed
Epoch  8,   450/  454; acc:  76.31; ppl:   2.99; 6329 src tok/s; 6554 tgt tok/s;     60 s elapsed
Train perplexity: 2.87296
Train accuracy: 77.0634
Validation perplexity: 6.18788
Validation accuracy: 68.9655
Decaying learning rate to 0.125

Epoch  9,    50/  454; acc:  79.31; ppl:   2.55; 6249 src tok/s; 6472 tgt tok/s;      7 s elapsed
Epoch  9,   100/  454; acc:  79.55; ppl:   2.48; 6364 src tok/s; 6614 tgt tok/s;     13 s elapsed
Epoch  9,   150/  454; acc:  79.28; ppl:   2.53; 6148 src tok/s; 6419 tgt tok/s;     20 s elapsed
Epoch  9,   200/  454; acc:  78.88; ppl:   2.62; 6322 src tok/s; 6566 tgt tok/s;     27 s elapsed
Epoch  9,   250/  454; acc:  79.30; ppl:   2.54; 6322 src tok/s; 6590 tgt tok/s;     33 s elapsed
Epoch  9,   300/  454; acc:  79.10; ppl:   2.56; 6389 src tok/s; 6617 tgt tok/s;     40 s elapsed
Epoch  9,   350/  454; acc:  78.74; ppl:   2.65; 6383 src tok/s; 6558 tgt tok/s;     47 s elapsed
Epoch  9,   400/  454; acc:  79.67; ppl:   2.49; 6295 src tok/s; 6557 tgt tok/s;     53 s elapsed
Epoch  9,   450/  454; acc:  79.06; ppl:   2.56; 6303 src tok/s; 6537 tgt tok/s;     60 s elapsed
Train perplexity: 2.55617
Train accuracy: 79.1861
Validation perplexity: 6.26534
Validation accuracy: 69.0861
Decaying learning rate to 0.0625

Epoch 10,    50/  454; acc:  80.32; ppl:   2.41; 6233 src tok/s; 6460 tgt tok/s;      7 s elapsed
Epoch 10,   100/  454; acc:  81.11; ppl:   2.30; 6291 src tok/s; 6531 tgt tok/s;     13 s elapsed
Epoch 10,   150/  454; acc:  81.50; ppl:   2.26; 6258 src tok/s; 6541 tgt tok/s;     20 s elapsed
Epoch 10,   200/  454; acc:  79.09; ppl:   2.54; 6396 src tok/s; 6590 tgt tok/s;     27 s elapsed
Epoch 10,   250/  454; acc:  80.10; ppl:   2.46; 6365 src tok/s; 6581 tgt tok/s;     33 s elapsed
Epoch 10,   300/  454; acc:  80.65; ppl:   2.35; 6391 src tok/s; 6650 tgt tok/s;     40 s elapsed
Epoch 10,   350/  454; acc:  80.24; ppl:   2.41; 6276 src tok/s; 6543 tgt tok/s;     46 s elapsed
Epoch 10,   400/  454; acc:  80.01; ppl:   2.45; 6304 src tok/s; 6529 tgt tok/s;     53 s elapsed
Epoch 10,   450/  454; acc:  80.18; ppl:   2.43; 6295 src tok/s; 6542 tgt tok/s;     60 s elapsed
Train perplexity: 2.40205
Train accuracy: 80.3298
Validation perplexity: 6.38896
Validation accuracy: 68.8875
Decaying learning rate to 0.03125

Epoch 11,    50/  454; acc:  81.25; ppl:   2.28; 6236 src tok/s; 6463 tgt tok/s;      7 s elapsed
Epoch 11,   100/  454; acc:  80.80; ppl:   2.35; 6402 src tok/s; 6641 tgt tok/s;     13 s elapsed
Epoch 11,   150/  454; acc:  80.49; ppl:   2.37; 6310 src tok/s; 6532 tgt tok/s;     20 s elapsed
Epoch 11,   200/  454; acc:  81.54; ppl:   2.30; 6231 src tok/s; 6532 tgt tok/s;     27 s elapsed
Epoch 11,   250/  454; acc:  80.33; ppl:   2.40; 6324 src tok/s; 6559 tgt tok/s;     33 s elapsed
Epoch 11,   300/  454; acc:  80.94; ppl:   2.30; 6411 src tok/s; 6680 tgt tok/s;     40 s elapsed
Epoch 11,   350/  454; acc:  80.56; ppl:   2.34; 6480 src tok/s; 6675 tgt tok/s;     47 s elapsed
Epoch 11,   400/  454; acc:  81.24; ppl:   2.26; 6212 src tok/s; 6457 tgt tok/s;     53 s elapsed
Epoch 11,   450/  454; acc:  80.40; ppl:   2.38; 6181 src tok/s; 6402 tgt tok/s;     60 s elapsed
Train perplexity: 2.33071
Train accuracy: 80.8412
Validation perplexity: 6.42322
Validation accuracy: 68.9513
Decaying learning rate to 0.015625

Epoch 12,    50/  454; acc:  81.40; ppl:   2.28; 6302 src tok/s; 6505 tgt tok/s;      7 s elapsed
Epoch 12,   100/  454; acc:  81.53; ppl:   2.27; 6379 src tok/s; 6587 tgt tok/s;     13 s elapsed
Epoch 12,   150/  454; acc:  80.96; ppl:   2.36; 6234 src tok/s; 6452 tgt tok/s;     20 s elapsed
Epoch 12,   200/  454; acc:  81.35; ppl:   2.25; 6294 src tok/s; 6543 tgt tok/s;     27 s elapsed
Epoch 12,   250/  454; acc:  80.95; ppl:   2.32; 6275 src tok/s; 6526 tgt tok/s;     33 s elapsed
Epoch 12,   300/  454; acc:  81.37; ppl:   2.26; 6444 src tok/s; 6682 tgt tok/s;     40 s elapsed
Epoch 12,   350/  454; acc:  80.95; ppl:   2.32; 6369 src tok/s; 6582 tgt tok/s;     47 s elapsed
Epoch 12,   400/  454; acc:  81.39; ppl:   2.25; 6165 src tok/s; 6461 tgt tok/s;     53 s elapsed
Epoch 12,   450/  454; acc:  81.27; ppl:   2.26; 6226 src tok/s; 6515 tgt tok/s;     60 s elapsed
Train perplexity: 2.29224
Train accuracy: 81.1854
Validation perplexity: 6.47112
Validation accuracy: 68.7314
Decaying learning rate to 0.0078125

Epoch 13,    50/  454; acc:  81.52; ppl:   2.26; 6347 src tok/s; 6560 tgt tok/s;      7 s elapsed
Epoch 13,   100/  454; acc:  81.22; ppl:   2.33; 6236 src tok/s; 6516 tgt tok/s;     13 s elapsed
Epoch 13,   150/  454; acc:  82.06; ppl:   2.21; 6229 src tok/s; 6490 tgt tok/s;     20 s elapsed
Epoch 13,   200/  454; acc:  80.89; ppl:   2.34; 6481 src tok/s; 6695 tgt tok/s;     27 s elapsed
Epoch 13,   250/  454; acc:  80.69; ppl:   2.38; 6384 src tok/s; 6604 tgt tok/s;     33 s elapsed
Epoch 13,   300/  454; acc:  81.83; ppl:   2.21; 6316 src tok/s; 6552 tgt tok/s;     40 s elapsed
Epoch 13,   350/  454; acc:  81.34; ppl:   2.29; 6283 src tok/s; 6512 tgt tok/s;     47 s elapsed
Epoch 13,   400/  454; acc:  81.46; ppl:   2.24; 6304 src tok/s; 6549 tgt tok/s;     53 s elapsed
Epoch 13,   450/  454; acc:  81.45; ppl:   2.27; 6381 src tok/s; 6620 tgt tok/s;     60 s elapsed
Train perplexity: 2.27648
Train accuracy: 81.4071
Validation perplexity: 6.47678
Validation accuracy: 68.8023
Decaying learning rate to 0.00390625

Epoch 14,    50/  454; acc:  81.42; ppl:   2.27; 6299 src tok/s; 6516 tgt tok/s;      7 s elapsed
Epoch 14,   100/  454; acc:  81.42; ppl:   2.26; 6305 src tok/s; 6546 tgt tok/s;     13 s elapsed
Epoch 14,   150/  454; acc:  81.14; ppl:   2.31; 6270 src tok/s; 6516 tgt tok/s;     20 s elapsed
Epoch 14,   200/  454; acc:  81.73; ppl:   2.22; 6343 src tok/s; 6580 tgt tok/s;     27 s elapsed
Epoch 14,   250/  454; acc:  81.76; ppl:   2.24; 6332 src tok/s; 6581 tgt tok/s;     33 s elapsed
Epoch 14,   300/  454; acc:  80.92; ppl:   2.30; 6401 src tok/s; 6626 tgt tok/s;     40 s elapsed
Epoch 14,   350/  454; acc:  81.11; ppl:   2.29; 6260 src tok/s; 6515 tgt tok/s;     47 s elapsed
Epoch 14,   400/  454; acc:  82.05; ppl:   2.18; 6310 src tok/s; 6583 tgt tok/s;     53 s elapsed
Epoch 14,   450/  454; acc:  81.21; ppl:   2.31; 6374 src tok/s; 6578 tgt tok/s;     60 s elapsed
Train perplexity: 2.26331
Train accuracy: 81.4384
Validation perplexity: 6.4905
Validation accuracy: 68.7881
Decaying learning rate to 0.00195312

Epoch 15,    50/  454; acc:  82.06; ppl:   2.19; 6118 src tok/s; 6394 tgt tok/s;      6 s elapsed
Epoch 15,   100/  454; acc:  80.70; ppl:   2.36; 6414 src tok/s; 6618 tgt tok/s;     13 s elapsed
Epoch 15,   150/  454; acc:  81.74; ppl:   2.22; 6280 src tok/s; 6542 tgt tok/s;     20 s elapsed
Epoch 15,   200/  454; acc:  80.88; ppl:   2.35; 6379 src tok/s; 6596 tgt tok/s;     27 s elapsed
Epoch 15,   250/  454; acc:  83.02; ppl:   2.09; 6220 src tok/s; 6501 tgt tok/s;     33 s elapsed
Epoch 15,   300/  454; acc:  80.32; ppl:   2.39; 6400 src tok/s; 6591 tgt tok/s;     40 s elapsed
Epoch 15,   350/  454; acc:  81.69; ppl:   2.23; 6246 src tok/s; 6498 tgt tok/s;     47 s elapsed
Epoch 15,   400/  454; acc:  80.99; ppl:   2.31; 6476 src tok/s; 6687 tgt tok/s;     53 s elapsed
Epoch 15,   450/  454; acc:  81.71; ppl:   2.23; 6306 src tok/s; 6566 tgt tok/s;     60 s elapsed
Train perplexity: 2.26177
Train accuracy: 81.4522
Validation perplexity: 6.49584
Validation accuracy: 68.781
Decaying learning rate to 0.000976562

Epoch 16,    50/  454; acc:  81.47; ppl:   2.27; 6367 src tok/s; 6615 tgt tok/s;      7 s elapsed
Epoch 16,   100/  454; acc:  81.59; ppl:   2.26; 6252 src tok/s; 6484 tgt tok/s;     13 s elapsed
Epoch 16,   150/  454; acc:  81.23; ppl:   2.29; 6375 src tok/s; 6600 tgt tok/s;     20 s elapsed
Epoch 16,   200/  454; acc:  81.98; ppl:   2.22; 6284 src tok/s; 6544 tgt tok/s;     27 s elapsed
Epoch 16,   250/  454; acc:  80.76; ppl:   2.35; 6371 src tok/s; 6593 tgt tok/s;     33 s elapsed
Epoch 16,   300/  454; acc:  81.90; ppl:   2.20; 6331 src tok/s; 6617 tgt tok/s;     40 s elapsed
Epoch 16,   350/  454; acc:  81.06; ppl:   2.33; 6461 src tok/s; 6660 tgt tok/s;     47 s elapsed
Epoch 16,   400/  454; acc:  82.28; ppl:   2.16; 6184 src tok/s; 6452 tgt tok/s;     53 s elapsed
Epoch 16,   450/  454; acc:  81.24; ppl:   2.26; 6307 src tok/s; 6533 tgt tok/s;     60 s elapsed
Train perplexity: 2.25984
Train accuracy: 81.5037
Validation perplexity: 6.49731
Validation accuracy: 68.781
Decaying learning rate to 0.000488281

Epoch 17,    50/  454; acc:  82.45; ppl:   2.14; 6152 src tok/s; 6473 tgt tok/s;      6 s elapsed
Epoch 17,   100/  454; acc:  80.25; ppl:   2.40; 6517 src tok/s; 6678 tgt tok/s;     13 s elapsed
Epoch 17,   150/  454; acc:  81.33; ppl:   2.27; 6415 src tok/s; 6621 tgt tok/s;     20 s elapsed
Epoch 17,   200/  454; acc:  81.81; ppl:   2.24; 6262 src tok/s; 6515 tgt tok/s;     26 s elapsed
Epoch 17,   250/  454; acc:  80.62; ppl:   2.39; 6369 src tok/s; 6597 tgt tok/s;     33 s elapsed
Epoch 17,   300/  454; acc:  82.75; ppl:   2.12; 6219 src tok/s; 6503 tgt tok/s;     40 s elapsed
Epoch 17,   350/  454; acc:  80.80; ppl:   2.35; 6381 src tok/s; 6572 tgt tok/s;     47 s elapsed
Epoch 17,   400/  454; acc:  82.56; ppl:   2.13; 6257 src tok/s; 6541 tgt tok/s;     53 s elapsed
Epoch 17,   450/  454; acc:  81.50; ppl:   2.26; 6260 src tok/s; 6497 tgt tok/s;     60 s elapsed
Train perplexity: 2.25832
Train accuracy: 81.5336
Validation perplexity: 6.49808
Validation accuracy: 68.781
Decaying learning rate to 0.000244141
